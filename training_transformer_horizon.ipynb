{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8559c452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml_models\n",
    "import data_pipeline2 as dp \n",
    "import numpy as np\n",
    "from training_models import train_transformer\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08ad117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'input_size' : 30,\n",
    "    'lr' : 1e-4,\n",
    "    'batch_size' : 32,\n",
    "    'hidden_size' : 8,\n",
    "    'output_size' : 3,\n",
    "    'layer_amt' : 3\n",
    "    }\n",
    "\n",
    "output_sizes = [1,5,10,20,40]\n",
    "input_sizes = [5,10,20,40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b2575f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_size': 5, 'lr': 0.0001, 'batch_size': 32, 'hidden_size': 8, 'output_size': 1, 'layer_amt': 3}\n",
      "RMSE, train 0.7124358174711348, test 1.3557102170682722\n",
      "RMSE, train 0.2934883598959729, test 1.1974666661793185\n",
      "RMSE, train 0.19634915013676105, test 1.1284217637392782\n",
      "RMSE, train 0.15384106921113055, test 1.1318252024150663\n",
      "RMSE, train 0.12557731392119714, test 1.1000876123866727\n",
      "RMSE, train 0.10419168093576733, test 1.0659940675381692\n",
      "RMSE, train 0.09093560919298131, test 1.06068188384656\n",
      "RMSE, train 0.07714660366695271, test 1.0521378127797958\n",
      "RMSE, train 0.06897657500808183, test 1.0436136030381726\n",
      "RMSE, train 0.06105835305107205, test 1.0118517695415405\n",
      "RMSE, train 0.05522307036481356, test 1.0344155877828598\n",
      "RMSE, train 0.048942792446540276, test 1.0283602903927527\n",
      "RMSE, train 0.045521107857509444, test 1.0332299420429814\n",
      "RMSE, train 0.039975390079331846, test 1.0247856229543686\n",
      "RMSE, train 0.03747431947131873, test 1.0406938774451133\n",
      "RMSE, train 0.03394884172561965, test 1.019019698423724\n",
      "RMSE, train 0.03097626727935766, test 1.0320837757279795\n",
      "RMSE, train 0.02748648299220876, test 1.0238990629872968\n",
      "RMSE, train 0.026142112948478978, test 1.0256391258970383\n",
      "RMSE, train 0.024024559172945235, test 1.01213279510698\n",
      "Early stopping at epoch 20 (no improvement for 10 epochs).\n",
      "Finished Training\n",
      "{'input_size': 10, 'lr': 0.0001, 'batch_size': 32, 'hidden_size': 8, 'output_size': 1, 'layer_amt': 3}\n",
      "RMSE, train 0.7495064170737016, test 1.270873496355104\n",
      "RMSE, train 0.22046595531315938, test 1.1455190097990116\n",
      "RMSE, train 0.15876400202271426, test 1.0827631083401767\n",
      "RMSE, train 0.12689075045860732, test 1.0506775228937795\n",
      "RMSE, train 0.10377952866708702, test 1.0446049748373425\n",
      "RMSE, train 0.08755280889631041, test 1.041114656393193\n",
      "RMSE, train 0.07754779968274991, test 1.0081301497772706\n",
      "RMSE, train 0.06830017437954784, test 1.0216803920170492\n",
      "RMSE, train 0.06194899822415611, test 0.9992686460826022\n",
      "RMSE, train 0.05569743948351396, test 1.0084818769092403\n",
      "RMSE, train 0.050536866720222874, test 0.9981016452647438\n",
      "RMSE, train 0.04514579885160392, test 1.01514121816178\n",
      "RMSE, train 0.04155379695146673, test 1.006206548410999\n",
      "RMSE, train 0.03804663298104848, test 0.9917076858114605\n",
      "RMSE, train 0.03343852941580268, test 0.9876171535204264\n",
      "RMSE, train 0.031000926907773806, test 0.989708334453835\n",
      "RMSE, train 0.02836438776691433, test 0.9909371984891655\n",
      "RMSE, train 0.02609560145385684, test 0.9913842377583842\n",
      "RMSE, train 0.023663810869068026, test 0.9926938870229012\n",
      "RMSE, train 0.021825685735173555, test 0.9996116190902458\n",
      "RMSE, train 0.020114811679333267, test 0.9877163356493327\n",
      "RMSE, train 0.01851409990698672, test 0.9875569382974924\n",
      "RMSE, train 0.01735245036281766, test 0.9878245582265303\n",
      "RMSE, train 0.015549068163839067, test 0.9883119789036837\n",
      "RMSE, train 0.014626578602709842, test 0.9801936807218662\n",
      "RMSE, train 0.01380456064692001, test 0.9833534305745905\n",
      "RMSE, train 0.012580626072486494, test 0.9907482985622627\n",
      "RMSE, train 0.011796556753935932, test 0.9834826623112702\n",
      "RMSE, train 0.011103571873339203, test 0.9767256090956286\n",
      "RMSE, train 0.01032932675424737, test 0.9890236746181141\n",
      "RMSE, train 0.00948149810718591, test 0.9821480118538722\n",
      "RMSE, train 0.008967002905596063, test 0.9796899905628409\n",
      "RMSE, train 0.008342739532565932, test 0.9887483804679114\n",
      "RMSE, train 0.007545593968094873, test 0.9825130508950919\n",
      "RMSE, train 0.007126600850611051, test 0.9788547631137627\n",
      "RMSE, train 0.006907893130546323, test 0.9823440796087596\n",
      "RMSE, train 0.006679583700733855, test 0.9827982039491007\n",
      "RMSE, train 0.006173945714998613, test 0.9798425189719713\n",
      "RMSE, train 0.0057267671107382396, test 0.9784543132486422\n",
      "Early stopping at epoch 39 (no improvement for 10 epochs).\n",
      "Finished Training\n",
      "{'input_size': 20, 'lr': 0.0001, 'batch_size': 32, 'hidden_size': 8, 'output_size': 1, 'layer_amt': 3}\n",
      "RMSE, train 0.3973413746017637, test 1.2488210726725428\n",
      "RMSE, train 0.23072497535552552, test 1.177169927902389\n",
      "RMSE, train 0.18538981744411911, test 1.1556832659662815\n",
      "RMSE, train 0.15577659809163638, test 1.078204522530238\n",
      "RMSE, train 0.12719084173917516, test 1.0781200122937822\n",
      "RMSE, train 0.10450157662952887, test 1.0398479182469218\n",
      "RMSE, train 0.08779584526666191, test 1.0263870941442357\n",
      "RMSE, train 0.07603062670638185, test 1.0085871475830412\n",
      "RMSE, train 0.06544466492638532, test 1.0096612298174907\n",
      "RMSE, train 0.057290759533326, test 1.0127117644276535\n",
      "RMSE, train 0.0508475491939895, test 0.992379192197532\n",
      "RMSE, train 0.04581925612506963, test 0.9927783624122017\n",
      "RMSE, train 0.03960201814214685, test 0.9993774807244017\n",
      "RMSE, train 0.03621547369123585, test 0.9849077452693069\n",
      "RMSE, train 0.03227811716774951, test 0.991767273137444\n",
      "RMSE, train 0.029798621583833243, test 0.9910297406870022\n",
      "RMSE, train 0.02632137014866193, test 0.9934826723316259\n",
      "RMSE, train 0.02487114264464963, test 0.9916743170796779\n",
      "RMSE, train 0.02302604095676719, test 0.9863800730621606\n",
      "RMSE, train 0.020181685928374465, test 0.9854673921016225\n",
      "RMSE, train 0.01907030987873006, test 0.978898810190067\n",
      "RMSE, train 0.017955774580364795, test 0.9845956122143227\n",
      "RMSE, train 0.016490092439485637, test 0.9841814239819845\n",
      "RMSE, train 0.015393883264911519, test 0.9852191973150822\n",
      "RMSE, train 0.01410273307644482, test 0.9867887230295884\n",
      "RMSE, train 0.012931009029735253, test 0.9874355322436282\n",
      "RMSE, train 0.012436640497519453, test 0.9871136012830233\n",
      "RMSE, train 0.011419830960469808, test 0.9832386782294825\n",
      "RMSE, train 0.01100754433486269, test 0.9839995377942136\n",
      "RMSE, train 0.010039701716287304, test 0.991357997321246\n",
      "RMSE, train 0.009539867060255013, test 0.985465827741121\n",
      "Early stopping at epoch 31 (no improvement for 10 epochs).\n",
      "Finished Training\n",
      "{'input_size': 40, 'lr': 0.0001, 'batch_size': 32, 'hidden_size': 8, 'output_size': 1, 'layer_amt': 3}\n",
      "RMSE, train 0.571195522256569, test 1.2650166311684776\n",
      "RMSE, train 0.29745044561018524, test 1.1777583600259294\n",
      "RMSE, train 0.2208769094901324, test 1.1836802088746838\n",
      "RMSE, train 0.1785679019116274, test 1.1236292664326875\n",
      "RMSE, train 0.14166246439043548, test 1.1197536266317554\n",
      "RMSE, train 0.11823421344161034, test 1.1082858127706192\n",
      "RMSE, train 0.09911888637768909, test 1.0749205742396561\n",
      "RMSE, train 0.08399050793030381, test 1.0523275773899228\n",
      "RMSE, train 0.07391914395006431, test 1.0428020249979169\n",
      "RMSE, train 0.06567804854549912, test 1.0544224802185507\n",
      "RMSE, train 0.05835651083535966, test 1.0300294318619896\n",
      "RMSE, train 0.051628426930962715, test 1.027426055249046\n",
      "RMSE, train 0.04744505607978683, test 1.0460945519746518\n",
      "RMSE, train 0.04252116552382921, test 1.026042657739976\n",
      "RMSE, train 0.039003335855243314, test 1.0369040814100527\n",
      "RMSE, train 0.035606370369741486, test 1.0216263836505366\n",
      "RMSE, train 0.03232081941726811, test 1.0326791826416464\n",
      "RMSE, train 0.028845428975925808, test 1.0325680863039166\n",
      "RMSE, train 0.02656963552950675, test 1.0322211606829774\n",
      "RMSE, train 0.02390324377270331, test 1.017484061273874\n",
      "RMSE, train 0.022146356133143984, test 1.0219911249244915\n",
      "RMSE, train 0.02023627665582911, test 1.0073421036495882\n",
      "RMSE, train 0.018262872465737254, test 1.020420213540395\n",
      "RMSE, train 0.01652164680123116, test 1.0173474129508524\n",
      "RMSE, train 0.015476155034788754, test 1.0004784534959232\n",
      "RMSE, train 0.014694813081021099, test 0.9981148346674209\n",
      "RMSE, train 0.012988817529099368, test 1.0025560966893738\n",
      "RMSE, train 0.012194121235354774, test 1.0017320449445761\n",
      "RMSE, train 0.011451802437075886, test 1.0035016373092054\n",
      "RMSE, train 0.010901626497329278, test 0.9961299808586345\n",
      "RMSE, train 0.010046036395539923, test 0.9895708555100011\n",
      "RMSE, train 0.009424884911395414, test 0.9953958634652343\n",
      "RMSE, train 0.008958634815950097, test 0.9912332389868942\n",
      "RMSE, train 0.008188528835613431, test 0.9912520492193746\n",
      "RMSE, train 0.008017390763591575, test 0.9984333909609738\n",
      "RMSE, train 0.0074593407587595015, test 0.9985448092806573\n",
      "RMSE, train 0.007082184103177527, test 0.9880336359435437\n",
      "RMSE, train 0.006757661651348727, test 0.9933951210741904\n",
      "RMSE, train 0.006280646897265621, test 0.9953001264263602\n",
      "RMSE, train 0.005854053805951198, test 0.9866212857704536\n",
      "RMSE, train 0.005703782519105564, test 1.0009352716745115\n",
      "RMSE, train 0.005246754714036649, test 0.9954345103572396\n",
      "RMSE, train 0.00505695891314295, test 0.9961871078201369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE, train 0.004757240352938774, test 0.9999981738773047\n",
      "RMSE, train 0.004516567262767554, test 0.9937204134230521\n",
      "RMSE, train 0.0043385552770573745, test 0.9977161971961751\n",
      "RMSE, train 0.004241401062909345, test 0.9933198684570836\n",
      "RMSE, train 0.00388343286698324, test 0.9964286725895077\n",
      "RMSE, train 0.003764565450334944, test 0.996181309223175\n",
      "RMSE, train 0.0035831039809779684, test 0.9925711493866116\n",
      "Early stopping at epoch 50 (no improvement for 10 epochs).\n",
      "Finished Training\n",
      "{'input_size': 5, 'lr': 0.0001, 'batch_size': 32, 'hidden_size': 8, 'output_size': 5, 'layer_amt': 3}\n",
      "RMSE, train 0.6164993587280473, test 1.3211484704135863\n",
      "RMSE, train 0.28275582931875703, test 1.2039782350713557\n",
      "RMSE, train 0.18183444456888304, test 1.13546744259921\n",
      "RMSE, train 0.13796230386792413, test 1.108734596859325\n",
      "RMSE, train 0.11325691030510972, test 1.0910627777911415\n",
      "RMSE, train 0.09650160880353782, test 1.0995444916496593\n",
      "RMSE, train 0.08283314029611047, test 1.101243064423238\n",
      "RMSE, train 0.07302306258239813, test 1.0719563364982605\n",
      "RMSE, train 0.06374323053751141, test 1.1223553281185055\n",
      "RMSE, train 0.057235395122740056, test 1.1055934724728924\n",
      "RMSE, train 0.05187157867476344, test 1.0824100744625753\n",
      "RMSE, train 0.04662996804672143, test 1.100613475831087\n",
      "RMSE, train 0.042088593059640016, test 1.0895579814418288\n",
      "RMSE, train 0.03916886749099039, test 1.1047581488435918\n",
      "RMSE, train 0.034703364451190516, test 1.0657385218241984\n",
      "RMSE, train 0.03243121479622899, test 1.064282008439056\n",
      "RMSE, train 0.030268060291306145, test 1.0477026937421687\n",
      "RMSE, train 0.02700812406575067, test 1.040039845241988\n",
      "RMSE, train 0.024551542190414284, test 1.0321629948852475\n",
      "RMSE, train 0.02213913820413572, test 1.0142440278668048\n",
      "RMSE, train 0.020186114773687515, test 1.0332731676003164\n",
      "RMSE, train 0.018718701267179342, test 1.02603690535569\n",
      "RMSE, train 0.01624402207145918, test 1.0087142781285214\n",
      "RMSE, train 0.015357492951631186, test 1.017158478744759\n",
      "RMSE, train 0.013780593001302481, test 1.0011305676018896\n",
      "RMSE, train 0.013143252635810284, test 1.0033127443849548\n",
      "RMSE, train 0.012093567459810255, test 0.9942240929308016\n",
      "RMSE, train 0.011180261608153101, test 0.9971861031429827\n",
      "RMSE, train 0.010178687566358055, test 0.9970280966482872\n",
      "RMSE, train 0.009305875886404406, test 0.9938822280276906\n",
      "RMSE, train 0.0085274923856618, test 0.9945928880498429\n",
      "RMSE, train 0.008029006177938783, test 0.9921392291045386\n",
      "RMSE, train 0.00741977357510252, test 0.9989589184276328\n",
      "RMSE, train 0.006957140832861525, test 0.9861432365642107\n",
      "RMSE, train 0.006520561764538739, test 0.9902663038781852\n",
      "RMSE, train 0.005977103279174997, test 0.9892969018171641\n",
      "RMSE, train 0.005581315277536745, test 0.9877526678822257\n",
      "RMSE, train 0.0051718076262454084, test 0.9875722601394022\n",
      "RMSE, train 0.004818135148173409, test 0.9881092713884085\n",
      "RMSE, train 0.004735743115053347, test 0.9806070088847609\n",
      "RMSE, train 0.00430352210875548, test 0.9859895504210606\n",
      "RMSE, train 0.004072374574353753, test 0.9835484461350874\n",
      "RMSE, train 0.0037671753504377368, test 0.9823454716974054\n",
      "RMSE, train 0.003660744546645982, test 0.9812467654874503\n",
      "RMSE, train 0.003338032444110966, test 0.9840258153016902\n",
      "RMSE, train 0.0031605696060597118, test 0.9817294828162706\n",
      "RMSE, train 0.0030921516983888125, test 0.9816425265359484\n",
      "RMSE, train 0.002902523936603346, test 0.9828370627292917\n",
      "RMSE, train 0.0026994814080910966, test 0.9821877932745563\n",
      "RMSE, train 0.00252270050517719, test 0.9826077390308222\n",
      "Early stopping at epoch 50 (no improvement for 10 epochs).\n",
      "Finished Training\n",
      "{'input_size': 10, 'lr': 0.0001, 'batch_size': 32, 'hidden_size': 8, 'output_size': 5, 'layer_amt': 3}\n",
      "RMSE, train 0.5849152277262251, test 1.295374380835032\n",
      "RMSE, train 0.3993841567931096, test 1.2913644606784238\n",
      "RMSE, train 0.31250398941645935, test 1.2134543243101088\n",
      "RMSE, train 0.2474093932720009, test 1.213228100437229\n",
      "RMSE, train 0.19986223404811435, test 1.1494499439910306\n",
      "RMSE, train 0.15840517121467215, test 1.0892592779660628\n",
      "RMSE, train 0.1296136554345981, test 1.0765892933990995\n",
      "RMSE, train 0.1115692002960473, test 1.0706692891100706\n",
      "RMSE, train 0.09785827394759605, test 1.0480792209253473\n",
      "RMSE, train 0.08562670558044487, test 1.0476233191409354\n",
      "RMSE, train 0.07672251408057641, test 1.0267616308341592\n",
      "RMSE, train 0.0695596061924026, test 1.018272200378321\n",
      "RMSE, train 0.06255980575657334, test 1.025921130079334\n",
      "RMSE, train 0.05626371204622151, test 1.0154208596480094\n",
      "RMSE, train 0.05200180458674623, test 1.016229682049509\n",
      "RMSE, train 0.04825683986798914, test 1.0076574094719806\n",
      "RMSE, train 0.04356219488280741, test 1.01298981349347\n",
      "RMSE, train 0.0400205016778859, test 1.0158361903691695\n",
      "RMSE, train 0.03682710439898074, test 1.0069850430650227\n",
      "RMSE, train 0.03470390079897051, test 1.0099944704670016\n",
      "RMSE, train 0.03252140917499696, test 0.9969280321719283\n",
      "RMSE, train 0.029419201174135174, test 0.9931397296614566\n",
      "RMSE, train 0.027349406256809955, test 0.9915586689771232\n",
      "RMSE, train 0.024800875932286102, test 0.997065698696395\n",
      "RMSE, train 0.023390187339256865, test 0.9944889042842187\n",
      "RMSE, train 0.022028131029483946, test 0.9932886084257546\n",
      "RMSE, train 0.019771213783720924, test 0.9914046273393146\n",
      "RMSE, train 0.019233468210917305, test 0.9969758755069668\n",
      "RMSE, train 0.018069729052122282, test 0.9912366766040608\n",
      "RMSE, train 0.016482704549004162, test 0.9859993162296586\n",
      "RMSE, train 0.015391906568480364, test 0.9939582155922712\n",
      "RMSE, train 0.014092588123734578, test 0.9932812704878339\n",
      "RMSE, train 0.013465515744856998, test 0.9912287067558806\n",
      "RMSE, train 0.01273013024893012, test 0.984458565206851\n",
      "RMSE, train 0.011981598990064976, test 0.9896892612263307\n",
      "RMSE, train 0.011495829849457753, test 0.9898480879048169\n",
      "RMSE, train 0.010915592602407945, test 0.9956298162371425\n",
      "RMSE, train 0.009926911299155393, test 0.9864895060911016\n",
      "RMSE, train 0.009818192397740724, test 0.9892799389564385\n",
      "RMSE, train 0.009159038253686552, test 0.9842637428792856\n",
      "RMSE, train 0.00876166164145751, test 0.9807079366203082\n",
      "RMSE, train 0.008095234589678165, test 0.9858964272474838\n",
      "RMSE, train 0.007917434807909145, test 0.9853832693928379\n",
      "RMSE, train 0.0076962386761611126, test 0.9819460627386125\n",
      "RMSE, train 0.007393965696769743, test 0.9826672183255017\n",
      "RMSE, train 0.006892529325146028, test 0.9869338612435228\n",
      "RMSE, train 0.006526146883102642, test 0.9832855372105614\n",
      "RMSE, train 0.006225747573439586, test 0.9823817974430019\n",
      "RMSE, train 0.0061196453002481605, test 0.9867046013221903\n",
      "RMSE, train 0.005571856857364043, test 0.9817454254223128\n",
      "RMSE, train 0.005521330257496712, test 0.9823567675331891\n",
      "Early stopping at epoch 51 (no improvement for 10 epochs).\n",
      "Finished Training\n",
      "{'input_size': 20, 'lr': 0.0001, 'batch_size': 32, 'hidden_size': 8, 'output_size': 5, 'layer_amt': 3}\n",
      "RMSE, train 0.6405582566276874, test 1.1993040788386549\n",
      "RMSE, train 0.3028658412088496, test 1.2065298333764076\n",
      "RMSE, train 0.22477990762210878, test 1.1259398949997765\n",
      "RMSE, train 0.1788342914144328, test 1.0760884380766325\n",
      "RMSE, train 0.14896915071443015, test 1.0410127565264702\n",
      "RMSE, train 0.12350836384043194, test 1.0468221109892641\n",
      "RMSE, train 0.10518146135838204, test 1.0295718205826623\n",
      "RMSE, train 0.09260429738978156, test 1.0196610492255007\n",
      "RMSE, train 0.08046154193336667, test 1.0137203785457782\n",
      "RMSE, train 0.07316967132775222, test 1.025124906961407\n",
      "RMSE, train 0.06648938089592914, test 1.0051100264702524\n",
      "RMSE, train 0.06037658705398408, test 1.0154317686600345\n",
      "RMSE, train 0.054590250578483726, test 1.0028403651501452\n",
      "RMSE, train 0.051175787048678124, test 1.0005488257322992\n",
      "RMSE, train 0.0460865990074112, test 1.0056210782911097\n",
      "RMSE, train 0.043580100729590794, test 1.0154901056417398\n",
      "RMSE, train 0.039907999505128605, test 0.9906314143112728\n",
      "RMSE, train 0.037414755982657276, test 1.0018119061631816\n",
      "RMSE, train 0.03408259782566809, test 1.005677386053971\n",
      "RMSE, train 0.032419561936006695, test 0.9997446781822613\n",
      "RMSE, train 0.0297396601175723, test 0.9977708575981004\n",
      "RMSE, train 0.028642168832630373, test 0.9894785159932715\n",
      "RMSE, train 0.025483185588737262, test 0.989194573036262\n",
      "RMSE, train 0.02387971559860023, test 0.9902643329863038\n",
      "RMSE, train 0.022598430360129717, test 0.992090851600681\n",
      "RMSE, train 0.02128717246869161, test 0.9892862646707467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE, train 0.019547564142905094, test 0.9884864035993814\n",
      "RMSE, train 0.018732377610013118, test 0.9837864600121975\n",
      "RMSE, train 0.01717891943720139, test 0.9941579539860997\n",
      "RMSE, train 0.016784414796114033, test 0.9923655159239259\n",
      "RMSE, train 0.015570758113511766, test 0.9907834311681134\n",
      "RMSE, train 0.014827416408279493, test 0.9858878063304084\n",
      "RMSE, train 0.013870664840779208, test 0.9971938838383981\n",
      "RMSE, train 0.013485686271197785, test 0.996298638571586\n",
      "RMSE, train 0.012254528338617132, test 0.9879670685955456\n",
      "RMSE, train 0.011746894966908335, test 0.9862885416618415\n",
      "RMSE, train 0.011081527646687815, test 0.9886212710823331\n",
      "RMSE, train 0.010454244581446831, test 0.9941216329378741\n",
      "Early stopping at epoch 38 (no improvement for 10 epochs).\n",
      "Finished Training\n",
      "{'input_size': 40, 'lr': 0.0001, 'batch_size': 32, 'hidden_size': 8, 'output_size': 5, 'layer_amt': 3}\n",
      "RMSE, train 0.5507444444961828, test 1.2813651134269406\n",
      "RMSE, train 0.2891007401686717, test 1.2783874809139906\n",
      "RMSE, train 0.2318451799871286, test 1.1653306050734087\n",
      "RMSE, train 0.19217302081855994, test 1.120786409486424\n",
      "RMSE, train 0.16065701023472842, test 1.0710800025198195\n",
      "RMSE, train 0.13053645823870427, test 1.0951802802808357\n",
      "RMSE, train 0.108349252920279, test 1.0911468911652613\n",
      "RMSE, train 0.09478349684940282, test 1.0500723520914714\n",
      "RMSE, train 0.08429051857796888, test 1.0746457552669024\n",
      "RMSE, train 0.07369663852351219, test 1.0531764331490103\n",
      "RMSE, train 0.06702379947004516, test 1.046273157452092\n",
      "RMSE, train 0.06169241538822359, test 1.0324784746073714\n",
      "RMSE, train 0.057230001260824774, test 1.0219410456190205\n",
      "RMSE, train 0.05083079087777942, test 1.0203199732785273\n",
      "RMSE, train 0.04833141682455475, test 1.003428168369062\n",
      "RMSE, train 0.043894211832242666, test 1.0135371733193446\n",
      "RMSE, train 0.04000727132345324, test 0.9961515863736471\n",
      "RMSE, train 0.037286929308020106, test 1.006091184688337\n",
      "RMSE, train 0.035311976850743255, test 0.9908729291925527\n",
      "RMSE, train 0.03314730790523416, test 1.00443305150427\n",
      "RMSE, train 0.029569330162990355, test 0.9924506737728311\n",
      "RMSE, train 0.027748630272821137, test 0.9921902695087471\n",
      "RMSE, train 0.026354876089576987, test 0.9918537892476477\n",
      "RMSE, train 0.023634028445683367, test 0.9919435044731757\n",
      "RMSE, train 0.022371355983859025, test 0.9913491383947507\n",
      "RMSE, train 0.02059157837801647, test 0.9946805256785769\n",
      "RMSE, train 0.01919874850737983, test 0.9936009820061501\n",
      "RMSE, train 0.018013916725251815, test 0.9978046501525725\n",
      "RMSE, train 0.016357840663892353, test 1.0095597969161139\n",
      "Early stopping at epoch 29 (no improvement for 10 epochs).\n",
      "Finished Training\n",
      "{'input_size': 5, 'lr': 0.0001, 'batch_size': 32, 'hidden_size': 8, 'output_size': 10, 'layer_amt': 3}\n",
      "RMSE, train 0.7008585301497259, test 1.2929620631670549\n",
      "RMSE, train 0.2596084200738629, test 1.1862798994880612\n",
      "RMSE, train 0.18664242658179162, test 1.1186691921646312\n",
      "RMSE, train 0.14380594039689903, test 1.0915076288126282\n",
      "RMSE, train 0.11652584915811365, test 1.0665200150619119\n",
      "RMSE, train 0.09715498738329519, test 1.056158058724161\n",
      "RMSE, train 0.08183915442825596, test 1.068111958645158\n",
      "RMSE, train 0.07224281702560088, test 1.088804859731157\n",
      "RMSE, train 0.06291053359866265, test 1.07183119477862\n",
      "RMSE, train 0.056267711079933426, test 1.0696466638375137\n",
      "RMSE, train 0.050837251192251263, test 1.0739488298610105\n",
      "RMSE, train 0.04604214015570739, test 1.0855359344159143\n",
      "RMSE, train 0.04035371702072049, test 1.0824190330202297\n",
      "RMSE, train 0.03832348715898789, test 1.0658719463873718\n",
      "RMSE, train 0.03438525656546074, test 1.0681588147151269\n",
      "RMSE, train 0.03132870357314294, test 1.071060327655178\n",
      "Early stopping at epoch 16 (no improvement for 10 epochs).\n",
      "Finished Training\n",
      "{'input_size': 10, 'lr': 0.0001, 'batch_size': 32, 'hidden_size': 8, 'output_size': 10, 'layer_amt': 3}\n",
      "RMSE, train 0.340478776161205, test 1.1116628281448198\n",
      "RMSE, train 0.18951901018366438, test 1.0771979534107705\n",
      "RMSE, train 0.14101525720390798, test 1.0623817998430003\n",
      "RMSE, train 0.11141295626664617, test 1.0223911228387252\n",
      "RMSE, train 0.09182585131242524, test 1.024385309478511\n",
      "RMSE, train 0.07898737427298051, test 1.0171316006909246\n",
      "RMSE, train 0.0666893036154172, test 1.009288712947265\n",
      "RMSE, train 0.059326516321984823, test 1.0076367616653443\n",
      "RMSE, train 0.05295118692503044, test 1.004756525029307\n",
      "RMSE, train 0.047134969523868614, test 1.0082444621169049\n",
      "RMSE, train 0.041767380670931054, test 1.0001939317454462\n",
      "RMSE, train 0.03763469728797421, test 0.9966162691945615\n",
      "RMSE, train 0.03486974163063962, test 0.999278666143832\n",
      "RMSE, train 0.03117415092632396, test 1.0031945539557416\n",
      "RMSE, train 0.029146214909535176, test 0.9971112873243249\n",
      "RMSE, train 0.026585805953545556, test 0.9901356497536535\n",
      "RMSE, train 0.024088837449575306, test 0.994594915535139\n",
      "RMSE, train 0.02271974981009106, test 1.0028579001841338\n",
      "RMSE, train 0.020886736971399332, test 0.991544532775879\n",
      "RMSE, train 0.019697486600915717, test 0.9950464549271957\n",
      "RMSE, train 0.01857988475456106, test 0.9879980437133623\n",
      "RMSE, train 0.018185568859725557, test 0.998928727792657\n",
      "RMSE, train 0.0169515121399945, test 0.9918206303016\n",
      "RMSE, train 0.01587404277780728, test 0.9903615961904111\n",
      "RMSE, train 0.014965988027778179, test 0.9949203260566877\n",
      "RMSE, train 0.01449671511947634, test 0.9930866808994957\n",
      "RMSE, train 0.013487224178408488, test 0.9858729414317919\n",
      "RMSE, train 0.012978998166578179, test 0.9876653790473938\n",
      "RMSE, train 0.012268254444430209, test 0.9875418149906656\n",
      "RMSE, train 0.011252835558553378, test 0.9875844732574794\n",
      "RMSE, train 0.011321839225931279, test 0.987748512496119\n",
      "RMSE, train 0.011225845650898297, test 0.9863108629765718\n",
      "RMSE, train 0.010781860277610347, test 0.9833048561344976\n",
      "RMSE, train 0.010528282959888157, test 0.9867261824400528\n",
      "RMSE, train 0.009865241124237355, test 0.980349252016648\n",
      "RMSE, train 0.00937127870272236, test 0.9920772171538809\n",
      "RMSE, train 0.009085322785855622, test 0.9821771450664686\n",
      "RMSE, train 0.008885016328222148, test 0.9842451276986496\n",
      "RMSE, train 0.008674724294206883, test 0.9848416968532231\n",
      "RMSE, train 0.00818307119057832, test 0.9860074045865432\n",
      "RMSE, train 0.008085149309682935, test 0.9851049319557522\n",
      "RMSE, train 0.00759024578670629, test 0.9782243780467821\n",
      "RMSE, train 0.007658537431529104, test 0.9916074592134226\n",
      "RMSE, train 0.007471968100511028, test 0.9850283026695251\n",
      "RMSE, train 0.007410650800976847, test 0.9824142917342807\n",
      "RMSE, train 0.006955677219033795, test 0.9846857340439507\n",
      "RMSE, train 0.006873729838465683, test 0.9820181014745132\n",
      "RMSE, train 0.00688247823387764, test 0.9846349946830584\n",
      "RMSE, train 0.006679960843393156, test 0.9845805440260016\n",
      "RMSE, train 0.0064056378143153405, test 0.9830918998821921\n",
      "RMSE, train 0.006204357320065126, test 0.9813124197980632\n",
      "RMSE, train 0.0060812228590629664, test 0.9793770349544028\n",
      "Early stopping at epoch 52 (no improvement for 10 epochs).\n",
      "Finished Training\n",
      "{'input_size': 20, 'lr': 0.0001, 'batch_size': 32, 'hidden_size': 8, 'output_size': 10, 'layer_amt': 3}\n",
      "RMSE, train 0.3478322076055768, test 1.1019017652634087\n",
      "RMSE, train 0.17039697241422308, test 1.0560140123061084\n",
      "RMSE, train 0.12963365286488437, test 1.0390983174700257\n",
      "RMSE, train 0.1003407404169173, test 1.0202920114228484\n",
      "RMSE, train 0.08163954515527984, test 1.0170446705380711\n",
      "RMSE, train 0.06743901763361933, test 1.0066488576591561\n",
      "RMSE, train 0.05668963306487408, test 1.000615148916157\n",
      "RMSE, train 0.04788016635222953, test 0.9796590233614685\n",
      "RMSE, train 0.04183304161167466, test 0.978492148425601\n",
      "RMSE, train 0.036741055105499625, test 0.9872900474508968\n",
      "RMSE, train 0.031254586920292524, test 0.9892382665511665\n",
      "RMSE, train 0.02789973802321628, test 0.988154241798121\n",
      "RMSE, train 0.02432393723777224, test 0.9815876598751873\n",
      "RMSE, train 0.021696911039950133, test 0.985533364869039\n",
      "RMSE, train 0.019450245663156156, test 0.9795728604727929\n",
      "RMSE, train 0.01699585994108342, test 0.9810678931551242\n",
      "RMSE, train 0.015666658846001588, test 0.9847934885856209\n",
      "RMSE, train 0.014226808915365772, test 0.9777554358364245\n",
      "RMSE, train 0.012583694270928912, test 0.9785213350156031\n",
      "RMSE, train 0.011588447575870851, test 0.9732683407604148\n",
      "RMSE, train 0.010154158190193465, test 0.97959089005759\n",
      "RMSE, train 0.009635906814255088, test 0.9793613088240317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE, train 0.008970652423191382, test 0.9793620525150124\n",
      "RMSE, train 0.008048849621466089, test 0.984496179523818\n",
      "RMSE, train 0.00712906351551046, test 0.9778268949701152\n",
      "RMSE, train 0.006628857277941393, test 0.9805488892651479\n",
      "RMSE, train 0.006268840394611366, test 0.9815198276021065\n",
      "RMSE, train 0.0060025728962826855, test 0.9804748920125699\n",
      "RMSE, train 0.005302889786520764, test 0.97551205043399\n",
      "RMSE, train 0.005415258888936196, test 0.9860400141925987\n",
      "Early stopping at epoch 30 (no improvement for 10 epochs).\n",
      "Finished Training\n",
      "{'input_size': 40, 'lr': 0.0001, 'batch_size': 32, 'hidden_size': 8, 'output_size': 10, 'layer_amt': 3}\n",
      "RMSE, train 0.5919507008535091, test 1.205574084694187\n",
      "RMSE, train 0.2701336754053229, test 1.1233271124462287\n",
      "RMSE, train 0.1981645209733585, test 1.1371605265885592\n",
      "RMSE, train 0.1577893380602502, test 1.0980198954542477\n",
      "RMSE, train 0.12411414237335475, test 1.0746891840050619\n",
      "RMSE, train 0.10405674220222716, test 1.0483808492620785\n",
      "RMSE, train 0.0896779199159055, test 1.0404813935359318\n",
      "RMSE, train 0.0783570564700046, test 1.0453060244520505\n",
      "RMSE, train 0.06972987589078268, test 1.0423849296445649\n",
      "RMSE, train 0.06045281491477532, test 1.0524685457348824\n",
      "RMSE, train 0.054781238172165674, test 1.0104638601963718\n",
      "RMSE, train 0.0483991638710252, test 1.0252849248548348\n",
      "RMSE, train 0.0428922967279725, test 1.0331173700590928\n",
      "RMSE, train 0.03849480371931662, test 1.032279225376745\n",
      "RMSE, train 0.03339473185637458, test 1.0302961835016806\n",
      "RMSE, train 0.030541718342444964, test 1.030868912115693\n",
      "RMSE, train 0.028055908946282786, test 1.0209706847866376\n",
      "RMSE, train 0.024346619658642495, test 1.024642704675595\n",
      "RMSE, train 0.02248314012787446, test 1.0065516053388517\n",
      "RMSE, train 0.02023911698617869, test 1.0056037207444508\n",
      "RMSE, train 0.01874963243284987, test 1.0089354533702135\n",
      "RMSE, train 0.016878793853325674, test 1.0194942944993575\n",
      "RMSE, train 0.015176189366041334, test 1.0155338154484828\n",
      "RMSE, train 0.01430837468436985, test 1.0113712064921856\n",
      "RMSE, train 0.013015751912719522, test 1.0022573433816433\n",
      "RMSE, train 0.012530241836558539, test 1.009347473581632\n",
      "RMSE, train 0.01127943896566226, test 1.0019831837465365\n",
      "RMSE, train 0.010685303590655552, test 1.0030928043027718\n",
      "RMSE, train 0.009547296987912344, test 1.0006601363420486\n",
      "RMSE, train 0.008832279178831313, test 1.0048434808850288\n",
      "RMSE, train 0.00805199293671364, test 1.0012651222447555\n",
      "RMSE, train 0.008104749109755938, test 1.0033283047378063\n",
      "RMSE, train 0.0073820922519033305, test 1.0030347142989438\n",
      "RMSE, train 0.006838257412186287, test 1.0001435801386833\n",
      "RMSE, train 0.0064160843612626195, test 0.9985181037336588\n",
      "RMSE, train 0.005810641070987766, test 0.9995725794384877\n",
      "RMSE, train 0.005541006373911814, test 1.000481575106581\n",
      "RMSE, train 0.005157114047147898, test 0.9955220924069484\n",
      "RMSE, train 0.004885566336481428, test 1.0006182081997395\n",
      "RMSE, train 0.0046315690998759386, test 0.9986303299665451\n",
      "RMSE, train 0.004316695520449712, test 0.9962205393239856\n",
      "RMSE, train 0.004129596392040828, test 0.994468999405702\n",
      "RMSE, train 0.003941617540678837, test 1.00047538553675\n",
      "RMSE, train 0.003611088622798861, test 1.0039571554710467\n",
      "RMSE, train 0.003493465121007628, test 0.9926242704192797\n",
      "RMSE, train 0.0032975300702600355, test 0.998751938653489\n",
      "RMSE, train 0.0031227141626372066, test 1.0016964155559738\n",
      "RMSE, train 0.00294662563284892, test 0.9947636937722564\n",
      "RMSE, train 0.0027859989491072624, test 0.9981443497041861\n",
      "RMSE, train 0.0025380707183920996, test 1.0040083422015111\n",
      "RMSE, train 0.0024379202929934525, test 0.9991719089448452\n",
      "RMSE, train 0.0023374488092035837, test 0.9992864349236091\n",
      "RMSE, train 0.0023227804582954073, test 0.9996682678659757\n",
      "RMSE, train 0.002239814569798505, test 0.9978488966201743\n",
      "RMSE, train 0.0020452748153316365, test 1.001466713224848\n",
      "Early stopping at epoch 55 (no improvement for 10 epochs).\n",
      "Finished Training\n",
      "{'input_size': 5, 'lr': 0.0001, 'batch_size': 32, 'hidden_size': 8, 'output_size': 20, 'layer_amt': 3}\n",
      "RMSE, train 0.9994227090822067, test 1.206119743841035\n",
      "RMSE, train 0.3465857613255515, test 1.261856984347105\n",
      "RMSE, train 0.2240408686576066, test 1.1616616797234331\n",
      "RMSE, train 0.16067050636009453, test 1.1217944922723941\n",
      "RMSE, train 0.12806194410344873, test 1.0525485295802355\n",
      "RMSE, train 0.10819293574423962, test 1.0311430847006184\n",
      "RMSE, train 0.0940150186979498, test 1.0231405166643006\n",
      "RMSE, train 0.08260135585137832, test 1.0125604213348456\n",
      "RMSE, train 0.07495769964275407, test 1.010619468986988\n",
      "RMSE, train 0.06588131844315653, test 1.0091713831893034\n",
      "RMSE, train 0.05870849141660339, test 1.0100258838917529\n",
      "RMSE, train 0.053278836163982324, test 1.0088531912437506\n",
      "RMSE, train 0.04778217562029954, test 0.9991595149040222\n",
      "RMSE, train 0.04396192452511172, test 0.9963696061500481\n",
      "RMSE, train 0.040141213981511804, test 0.9968706627509424\n",
      "RMSE, train 0.036562291000338684, test 0.9826596075935023\n",
      "RMSE, train 0.033033004156384854, test 0.9999584262924535\n",
      "RMSE, train 0.02992360800309898, test 0.993712682010872\n",
      "RMSE, train 0.02715854931111429, test 0.9976407906838826\n",
      "RMSE, train 0.026143268989459945, test 0.9918970417763505\n",
      "RMSE, train 0.02394380196036207, test 0.9946905081825597\n",
      "RMSE, train 0.022620340377043666, test 0.9922250929687705\n",
      "RMSE, train 0.020469345238611655, test 0.9947778007813862\n",
      "RMSE, train 0.01905798528664025, test 0.9855357059942824\n",
      "RMSE, train 0.01788187229271353, test 0.9828903257314648\n",
      "RMSE, train 0.016945011154920968, test 0.9883640988596848\n",
      "Early stopping at epoch 26 (no improvement for 10 epochs).\n",
      "Finished Training\n",
      "{'input_size': 10, 'lr': 0.0001, 'batch_size': 32, 'hidden_size': 8, 'output_size': 20, 'layer_amt': 3}\n",
      "RMSE, train 0.49595907928564087, test 1.1555746875771689\n",
      "RMSE, train 0.2071585927342353, test 1.0939962032738082\n",
      "RMSE, train 0.15042746204030888, test 1.0315306905759585\n",
      "RMSE, train 0.12087361232248123, test 1.0489069781172167\n",
      "RMSE, train 0.09889525449479776, test 1.0320635708647037\n",
      "RMSE, train 0.08389941102620464, test 1.0208770044899862\n",
      "RMSE, train 0.07274361990074807, test 1.0149099646358315\n",
      "RMSE, train 0.062338677957453535, test 1.0094417869497876\n",
      "RMSE, train 0.05336942892070576, test 0.9985032562815815\n",
      "RMSE, train 0.04804538068415873, test 0.9996807504137721\n",
      "RMSE, train 0.04228981150389386, test 0.9988497615954198\n",
      "RMSE, train 0.03803522512316704, test 0.9975332694316129\n",
      "RMSE, train 0.03363775503466682, test 0.9896537557654425\n",
      "RMSE, train 0.030400002011904, test 0.9998518244935832\n",
      "RMSE, train 0.027560582465905648, test 0.9911778273932431\n",
      "RMSE, train 0.025570841892434477, test 0.997347989213576\n",
      "RMSE, train 0.021943993364239422, test 0.9860667859742401\n",
      "RMSE, train 0.020164291040278604, test 0.9889689126145949\n",
      "RMSE, train 0.018441893932965036, test 0.9931681194436659\n",
      "RMSE, train 0.017354937529216432, test 0.9798029633837009\n",
      "RMSE, train 0.015892415365930897, test 0.982304300190112\n",
      "RMSE, train 0.014510117338097449, test 0.9804823453273248\n",
      "RMSE, train 0.013455370021879572, test 0.9872045757573679\n",
      "RMSE, train 0.012408256158545914, test 0.9784543407619546\n",
      "RMSE, train 0.011700802216423266, test 0.980321184210821\n",
      "RMSE, train 0.010732333562670969, test 0.9788935094251545\n",
      "RMSE, train 0.010125101254751797, test 0.9792453972024655\n",
      "RMSE, train 0.009683055682588084, test 0.9703243123281986\n",
      "RMSE, train 0.008611248663601554, test 0.9865865133224277\n",
      "RMSE, train 0.008320463983124882, test 0.9760088264395338\n",
      "RMSE, train 0.007407457720709655, test 0.9833496907435426\n",
      "RMSE, train 0.0073877150159446115, test 0.9813790239325357\n",
      "RMSE, train 0.006634380166197505, test 0.9770169219839464\n",
      "RMSE, train 0.00649549142559865, test 0.9798560208136883\n",
      "RMSE, train 0.006179440079565941, test 0.9737212269678028\n",
      "RMSE, train 0.005695265776012093, test 0.9792819460597607\n",
      "RMSE, train 0.005331797767775092, test 0.975124618875871\n",
      "RMSE, train 0.004952211860638329, test 0.9770631527681963\n",
      "Early stopping at epoch 38 (no improvement for 10 epochs).\n",
      "Finished Training\n",
      "{'input_size': 20, 'lr': 0.0001, 'batch_size': 32, 'hidden_size': 8, 'output_size': 20, 'layer_amt': 3}\n",
      "RMSE, train 0.362853123463531, test 1.24023627714046\n",
      "RMSE, train 0.20289968136441397, test 1.1713944974454862\n",
      "RMSE, train 0.1619349805784197, test 1.1430898077279619\n",
      "RMSE, train 0.13486088140737132, test 1.0933470739033615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE, train 0.11427567601469378, test 1.0652422361726899\n",
      "RMSE, train 0.09812937124749543, test 1.0641283091989535\n",
      "RMSE, train 0.0852261446283648, test 1.0622144005831005\n",
      "RMSE, train 0.07380567976846547, test 1.038022314749875\n",
      "RMSE, train 0.06474626060761068, test 1.0372356060639167\n",
      "RMSE, train 0.055919516406900245, test 1.0389735768142256\n",
      "RMSE, train 0.050660773368459415, test 1.0358136255764268\n",
      "RMSE, train 0.04625332137158322, test 1.022402395033142\n",
      "RMSE, train 0.04095436967112956, test 1.0217545509917065\n",
      "RMSE, train 0.03848993288454845, test 1.0205387328434916\n",
      "RMSE, train 0.03521789438721686, test 1.0081735623693002\n",
      "RMSE, train 0.031204763771147597, test 1.003611283105554\n",
      "RMSE, train 0.028730433783770458, test 1.0170222381943639\n",
      "RMSE, train 0.026882610303701527, test 1.0047804186645064\n",
      "RMSE, train 0.02493612609157724, test 1.0079723613210094\n",
      "RMSE, train 0.02432797548323516, test 1.0108812153339386\n",
      "RMSE, train 0.02281473587309132, test 1.0247992601209475\n",
      "RMSE, train 0.02029819764426938, test 1.0108141638700245\n",
      "RMSE, train 0.020233413955581654, test 1.0071658622292639\n",
      "RMSE, train 0.019330045141969895, test 0.9967915488099589\n",
      "RMSE, train 0.01730888030843504, test 0.9996420307066834\n",
      "RMSE, train 0.017139107130485588, test 0.9922744450927938\n",
      "RMSE, train 0.015900525305367067, test 1.0153022966917278\n",
      "RMSE, train 0.015849293538119265, test 0.9967678715881793\n",
      "RMSE, train 0.014500246892048315, test 0.9982650900639377\n",
      "RMSE, train 0.013625448394894175, test 0.998088786324251\n",
      "RMSE, train 0.013582851252719902, test 1.0253567435209034\n",
      "RMSE, train 0.013242587957624124, test 0.9984753178161325\n",
      "RMSE, train 0.012589379559954598, test 1.00968678657291\n",
      "RMSE, train 0.011935051444325056, test 0.9890854192300907\n",
      "RMSE, train 0.011519043951908087, test 1.001895668147837\n",
      "RMSE, train 0.01111925670638185, test 0.9953246735832066\n",
      "RMSE, train 0.01063059821322277, test 0.9905652157889987\n",
      "RMSE, train 0.009795636381077936, test 1.0172488339317656\n",
      "RMSE, train 0.00983274091950965, test 0.9933672961100791\n",
      "RMSE, train 0.009841358266447863, test 0.9888541572591634\n",
      "RMSE, train 0.009075400758547942, test 1.0117762881575278\n",
      "RMSE, train 0.008831731618919593, test 0.996462692334814\n",
      "RMSE, train 0.008733352472576492, test 1.0037841941546468\n",
      "RMSE, train 0.00834100570578367, test 1.0095638488102885\n",
      "RMSE, train 0.00831751210444702, test 1.0043845275073375\n",
      "RMSE, train 0.007919273754546414, test 0.9875287734767766\n",
      "RMSE, train 0.007433379337472527, test 0.9803828533413341\n",
      "RMSE, train 0.007319569349899499, test 0.990684066582652\n",
      "RMSE, train 0.007295144079783998, test 0.9959549273102029\n",
      "RMSE, train 0.007223541990769784, test 0.9909407591935501\n",
      "RMSE, train 0.006893398298831068, test 1.0032383342391078\n",
      "RMSE, train 0.0066669032463224295, test 0.9970795321233065\n",
      "RMSE, train 0.0067201883705209, test 1.0042745557803552\n",
      "RMSE, train 0.006454779299456104, test 0.9912850769978125\n",
      "RMSE, train 0.0062632966789941255, test 0.9997315097202375\n",
      "RMSE, train 0.006574696578879591, test 0.9962390695960777\n",
      "RMSE, train 0.005871269894783737, test 0.9861794183555158\n",
      "Early stopping at epoch 57 (no improvement for 10 epochs).\n",
      "Finished Training\n",
      "{'input_size': 40, 'lr': 0.0001, 'batch_size': 32, 'hidden_size': 8, 'output_size': 20, 'layer_amt': 3}\n",
      "RMSE, train 0.3780019077851445, test 1.184164203537835\n",
      "RMSE, train 0.22943778946553922, test 1.206531220012241\n",
      "RMSE, train 0.1804425644505056, test 1.232626094420751\n",
      "RMSE, train 0.1439385476077824, test 1.2222731984323925\n",
      "RMSE, train 0.1181385425544492, test 1.1946511487166087\n",
      "RMSE, train 0.09608400808873202, test 1.2342532455921174\n",
      "RMSE, train 0.08180336014560934, test 1.179447540309694\n",
      "RMSE, train 0.07167547363336035, test 1.1290771199597254\n",
      "RMSE, train 0.06496905599723286, test 1.0923173586527506\n",
      "RMSE, train 0.060081250709544294, test 1.1004236506091223\n",
      "RMSE, train 0.05413646186448413, test 1.0939049038622115\n",
      "RMSE, train 0.0475330953297991, test 1.0711146510309644\n",
      "RMSE, train 0.044505946621841014, test 1.0711697118149863\n",
      "RMSE, train 0.04021668024712576, test 1.0830912517176734\n",
      "RMSE, train 0.03844119254700498, test 1.05537814895312\n",
      "RMSE, train 0.03472179506285332, test 1.0527807421154447\n",
      "RMSE, train 0.03263028019491954, test 1.0408593495686849\n",
      "RMSE, train 0.029983366086496658, test 1.0321756614579094\n",
      "RMSE, train 0.027309894129995387, test 1.0302930341826544\n",
      "RMSE, train 0.025831264463777974, test 1.0434819396999147\n",
      "RMSE, train 0.023393634572434378, test 1.026028976837794\n",
      "RMSE, train 0.023029979893239842, test 1.0184209280543857\n",
      "RMSE, train 0.021430790573709977, test 1.0283821172184415\n",
      "RMSE, train 0.019748556224323026, test 1.0211985780133142\n",
      "RMSE, train 0.018616469225512362, test 1.0149249103334215\n",
      "RMSE, train 0.017228854330623086, test 1.0130165537198386\n",
      "RMSE, train 0.016867583946417766, test 1.019417565398746\n",
      "RMSE, train 0.015553350640696817, test 1.0205990042951372\n",
      "RMSE, train 0.014625107725163435, test 1.010687545273039\n",
      "RMSE, train 0.013504341802612509, test 1.0103611724244224\n",
      "RMSE, train 0.012751199178287202, test 1.016663999027676\n",
      "RMSE, train 0.012320893567502298, test 1.0202842354774475\n",
      "RMSE, train 0.011724854199431655, test 1.0128000968032413\n",
      "RMSE, train 0.011025752683955864, test 1.0198501812087164\n",
      "RMSE, train 0.010495873421023918, test 1.01447527607282\n",
      "RMSE, train 0.009632427798272825, test 1.0099801460901896\n",
      "RMSE, train 0.009467486612815058, test 1.0192171686225466\n",
      "RMSE, train 0.009281865846380752, test 1.0194993827078078\n",
      "RMSE, train 0.008641095722792043, test 1.0135614501105414\n",
      "RMSE, train 0.00809823007779723, test 1.0159108224842284\n",
      "RMSE, train 0.008111609911013926, test 1.0152695867750379\n",
      "RMSE, train 0.007376978646102801, test 1.011051392555237\n",
      "RMSE, train 0.007067527232259873, test 1.01357797715399\n",
      "RMSE, train 0.006669006386340026, test 1.0161536143885719\n",
      "RMSE, train 0.006183798564221419, test 1.0143112623029285\n",
      "RMSE, train 0.006057887434120971, test 1.0120532774262958\n",
      "Early stopping at epoch 46 (no improvement for 10 epochs).\n",
      "Finished Training\n",
      "{'input_size': 5, 'lr': 0.0001, 'batch_size': 32, 'hidden_size': 8, 'output_size': 40, 'layer_amt': 3}\n",
      "RMSE, train 0.5630442282426328, test 1.222920637540143\n",
      "RMSE, train 0.22984161472043665, test 1.1379509658524485\n",
      "RMSE, train 0.1678330639245166, test 1.1162483788499928\n",
      "RMSE, train 0.1265969913370889, test 1.0770458452629321\n",
      "RMSE, train 0.10351492091102531, test 1.0278110004434682\n",
      "RMSE, train 0.0865078825448938, test 1.0432250674324806\n",
      "RMSE, train 0.07431256667253149, test 1.0408654231013674\n",
      "RMSE, train 0.06291568547505476, test 1.0119199776890302\n",
      "RMSE, train 0.0565211779021255, test 1.0246711575021648\n",
      "RMSE, train 0.049685279704393556, test 1.0230242047044966\n",
      "RMSE, train 0.045297514567547874, test 1.0140982336468167\n",
      "RMSE, train 0.041384002469334304, test 1.0063989583892052\n",
      "RMSE, train 0.03810989506961664, test 1.0273885498143205\n",
      "RMSE, train 0.03338114764259031, test 1.030922409861979\n",
      "RMSE, train 0.031073385492094718, test 1.0283297246152705\n",
      "RMSE, train 0.028569973445062534, test 1.038490238815847\n",
      "RMSE, train 0.027081516793357494, test 1.0234667448082355\n",
      "RMSE, train 0.02382606147699659, test 1.0441595364098597\n",
      "RMSE, train 0.022432763066853243, test 1.0164339882556839\n",
      "RMSE, train 0.020668968500246118, test 1.0311940098651733\n",
      "RMSE, train 0.019333374729803548, test 1.0212441661743203\n",
      "RMSE, train 0.018318877296982228, test 1.03129438197974\n",
      "Early stopping at epoch 22 (no improvement for 10 epochs).\n",
      "Finished Training\n",
      "{'input_size': 10, 'lr': 0.0001, 'batch_size': 32, 'hidden_size': 8, 'output_size': 40, 'layer_amt': 3}\n",
      "RMSE, train 0.7053643922613124, test 1.1862219087779522\n",
      "RMSE, train 0.365165049153747, test 1.2125639244914055\n",
      "RMSE, train 0.2654829803628452, test 1.0954809542745352\n",
      "RMSE, train 0.20332058828625113, test 1.0999352255215247\n",
      "RMSE, train 0.1625883358370776, test 1.078935840477546\n",
      "RMSE, train 0.1348294410572359, test 1.055388907591502\n",
      "RMSE, train 0.11553856907318337, test 1.0631991295764844\n",
      "RMSE, train 0.10012340060237682, test 1.0440581534057856\n",
      "RMSE, train 0.08693252372169735, test 1.0507426013549168\n",
      "RMSE, train 0.07837227580015256, test 1.0217894557863474\n",
      "RMSE, train 0.07120945531348086, test 1.0347452809413273\n",
      "RMSE, train 0.06170200811454443, test 1.0380071876570582\n",
      "RMSE, train 0.057915665759620344, test 1.0323326146850984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE, train 0.051373707346919206, test 1.0288994483028848\n",
      "RMSE, train 0.04779376217276987, test 1.037693949105839\n",
      "RMSE, train 0.04395921049978246, test 1.0458045176540811\n",
      "RMSE, train 0.03963020874300238, test 1.044530554053684\n",
      "RMSE, train 0.03702640221595312, test 1.0509360128392775\n",
      "RMSE, train 0.033736240859566766, test 1.04166238134106\n",
      "RMSE, train 0.03228053801711837, test 1.0365434056147933\n",
      "Early stopping at epoch 20 (no improvement for 10 epochs).\n",
      "Finished Training\n",
      "{'input_size': 20, 'lr': 0.0001, 'batch_size': 32, 'hidden_size': 8, 'output_size': 40, 'layer_amt': 3}\n",
      "RMSE, train 0.6491061582799871, test 1.1666620936658647\n",
      "RMSE, train 0.2927764077793877, test 1.1778790566656325\n",
      "RMSE, train 0.20272324306543946, test 1.2411310977405972\n",
      "RMSE, train 0.15685345055761363, test 1.1544733769363826\n",
      "RMSE, train 0.12912954381011887, test 1.1790170636441972\n",
      "RMSE, train 0.10740582969270626, test 1.1189434859487746\n",
      "RMSE, train 0.09196045757283901, test 1.1133971677886114\n",
      "RMSE, train 0.0785275813232294, test 1.0789912197324965\n",
      "RMSE, train 0.06992907790000226, test 1.0780314326286315\n",
      "RMSE, train 0.06293972945618984, test 1.057867815097173\n",
      "RMSE, train 0.05644591950944492, test 1.0485197948084937\n",
      "RMSE, train 0.05099211808926654, test 1.0493329415718715\n",
      "RMSE, train 0.04723148263308077, test 1.0583357029491\n",
      "RMSE, train 0.04256200953072936, test 1.031547227833006\n",
      "RMSE, train 0.03966233785944486, test 1.0475779228740267\n",
      "RMSE, train 0.03576672389962763, test 1.0400668263435364\n",
      "RMSE, train 0.033163862963109164, test 1.0269207663006252\n",
      "RMSE, train 0.03078796280261802, test 1.0334563202328153\n",
      "RMSE, train 0.027761614427090978, test 1.007182905409071\n",
      "RMSE, train 0.025623410325145948, test 1.0184836831357744\n",
      "RMSE, train 0.02327253320944719, test 1.0112969590557945\n",
      "RMSE, train 0.021851385856986125, test 1.0328364060984718\n",
      "RMSE, train 0.02052960071004827, test 1.0262090312110053\n",
      "RMSE, train 0.019759840069644936, test 1.0163208524386087\n",
      "RMSE, train 0.018348622091887694, test 1.0211467262771394\n",
      "RMSE, train 0.016792119311224016, test 1.0143098566267226\n",
      "RMSE, train 0.016271395300015444, test 1.0275754223267237\n",
      "RMSE, train 0.014581200870060214, test 1.0253145509295993\n",
      "RMSE, train 0.014184096544438777, test 1.027863190571467\n",
      "Early stopping at epoch 29 (no improvement for 10 epochs).\n",
      "Finished Training\n",
      "{'input_size': 40, 'lr': 0.0001, 'batch_size': 32, 'hidden_size': 8, 'output_size': 40, 'layer_amt': 3}\n",
      "RMSE, train 0.5681093927484435, test 1.241062764173899\n",
      "RMSE, train 0.3563933461161789, test 1.2494872174201868\n",
      "RMSE, train 0.2717491546384642, test 1.219535482999606\n",
      "RMSE, train 0.21390796157986947, test 1.223118156194687\n",
      "RMSE, train 0.17597213357315628, test 1.1331704436586454\n",
      "RMSE, train 0.1491348259847298, test 1.1341321884821622\n",
      "RMSE, train 0.13511641819111284, test 1.0582740291571007\n",
      "RMSE, train 0.11248455392748025, test 1.0900581761812553\n",
      "RMSE, train 0.10219282315132959, test 1.0427049182546444\n",
      "RMSE, train 0.09009272746764982, test 1.0792166170401452\n",
      "RMSE, train 0.08136344395833224, test 1.0506361829928863\n",
      "RMSE, train 0.07875493253549311, test 1.0465561197354243\n",
      "RMSE, train 0.07069453706158284, test 1.0264457341952202\n",
      "RMSE, train 0.06763982145562536, test 1.0259967687038274\n",
      "RMSE, train 0.062212293961802, test 1.027511240580143\n",
      "RMSE, train 0.05819086394507751, test 1.0342310567696889\n",
      "RMSE, train 0.054503899351969316, test 1.0244082418771892\n",
      "RMSE, train 0.05013092918280872, test 1.0167974474338384\n",
      "RMSE, train 0.047498910934475724, test 1.0372274403388684\n",
      "RMSE, train 0.044099983028092676, test 1.0409190708245986\n",
      "RMSE, train 0.04147434951824563, test 1.019708881011376\n",
      "RMSE, train 0.03884092856175339, test 1.053147873817346\n",
      "RMSE, train 0.037971544299982604, test 1.0239462539171562\n",
      "RMSE, train 0.03588783106479411, test 1.0423149241086764\n",
      "RMSE, train 0.03400827361984509, test 1.0206033197733073\n",
      "RMSE, train 0.031161980810821797, test 1.0269969755258315\n",
      "RMSE, train 0.030000029140474085, test 1.0561981499195099\n",
      "RMSE, train 0.02812804945532778, test 1.0353930248663976\n",
      "Early stopping at epoch 28 (no improvement for 10 epochs).\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for output in output_sizes:\n",
    "\n",
    "    config['output_size'] = output\n",
    "\n",
    "    for inputs in input_sizes:\n",
    "\n",
    "        config['input_size'] = inputs    \n",
    "\n",
    "        print(config)\n",
    "\n",
    "        mod = train_transformer(config, 1000,data_dir = 'data_synthetic', only_one = True)\n",
    "\n",
    "        torch.save(obj = mod.state_dict(), f = 'models/transformer/'+'output_size' + str(output) + 'input_size' + str(inputs) + '_singlevar.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0306f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()       \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "    \n",
    "    \n",
    "class TransAm(nn.Module):\n",
    "    def __init__(self, feature_size=250, num_layers=3, dropout=0.1):\n",
    "        super(TransAm, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(feature_size)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n",
    "        self.decoder = nn.Linear(feature_size,1)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1    \n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self,src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src,self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d483e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
